#This is a draft idea of implementing https://github.com/google-gemini/gemini-cli/discussions/3316#discussioncomment-13672455 and https://github.com/google-gemini/gemini-cli/discussions/2386 PMBOK-like rigour on AI's thinking so that they solve some puzzles better, like PM tasks/ 


#ver. 0.4

See official info about settings levels: https://github.com/google-gemini/gemini-cli/blob/main/docs/cli/configuration.md

ğŸ“š Gemini CLI â€” Four Layers of Memory
Gemini CLI doesnâ€™t â€œrememberâ€ â€” it assembles artificial memory by stacking system-level constraints, hard-coded prompts, and semi-persistent disk files. You, the user, decide how deep that stack goes.

ğŸ§© Level 1 â€” Reinforcement Learning from Human Feedback (RLHF)
What it is:
Soft-wired behavioral conditioning baked into the model weights and the CLI orchestration code. Think of it as embedded etiquette plus non-negotiable reflexes:

â€œAlways verify output before finishing.â€

â€œAlways check the memory bank first.â€

â€œDefault to safer actions (e.g., MCP server) instead of rm -rf.â€

Key point:
This layer lives partly in the underlying LLM training (LoRA-like fine-tuning) and partly in the CLIâ€™s code wrapper â€” you canâ€™t skip it. Itâ€™s the autopilot guardrails.

ğŸ§© Level 2 â€” System Prompt
What it is:
A hard-coded root directive bundled in the CLIâ€™s source. This is the prime law the LLM sees at the start of every run:

â€œYou are Gemini CLI. You must follow user instructions exactly. You must verify output. You must prefer approved subsystems over direct shell commands.â€

Key point:
If RLHF is the reflex, this is the explicit law â€” the bedrock override that shapes every single request, no matter what files or session context you add later.

ğŸ§© Level 3 â€” Gemini.md Files
What it is:
Semi-persistent configuration and context docs. These live on disk and always get read at runtime.

Global .config version â†’ applies to all projects.

Project-local version â†’ scoped to your current working directory.

Key point:
You or Gemini can edit these mid-run (save/update). They are forcibly stacked into the prompt every call â€” which means they simulate â€œlong-termâ€ memory. They are the memory, in practice. 

Read about them here: https://github.com/Manamama/Puzzles_for_AIs/blob/main/docs/pmbok_instructions/ideas/GEMINI%20CLI%20tricks.md or ask https://askdev.ai/github/google-gemini/gemini-cli?trk=public_post_comment-text

ğŸ§© Level 4 â€” Expanded Memory Bank (Proposal)
What it is:
A user/community-driven pattern to break memory into specialized, modular context files:

GEMINI-codebase.md â†’ maps file structure, key APIs.

GEMINI-activeContext.md â†’ session goals, TODOs, scratchpad.

GEMINI-patterns.md â†’ style rules, repeatable solutions.

GEMINI-decisions.md â†’ design rationales, tradeoffs.

GEMINI-troubleshooting.md â†’ known issues, proven fixes.

GEMINI-config-variables.md â†’ config keys, env refs.

Key point:
When you run Gemini CLI, it can stack all these files â†’ unify them into one superprompt â†’ pass that to the LLM. This turns scattered disk files into a primitive modular knowledge base â€” which the model must ingest fresh each run.

Net effect:
You push the limits of whatâ€™s possible inside a finite context window by shifting â€œstatefulnessâ€ to the file system, not the ephemeral chat buffer.

âœ… Why This Matters
Gemini CLIâ€™s real trick isnâ€™t magic memory â€” itâ€™s disciplined prompt orchestration.

Levels 1â€“3 are guaranteed by design.

Level 4 is your hackable frontier: more files, smarter splits, tighter scope.

Use it well, and you turn a stateless LLM into a semi-stateful project agent â€” one .md chunk at a time.



